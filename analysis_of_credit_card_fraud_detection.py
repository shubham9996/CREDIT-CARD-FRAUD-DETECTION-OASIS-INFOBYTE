# -*- coding: utf-8 -*-
"""Analysis of Credit Card Fraud Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/analysis-of-credit-card-fraud-detection-9a42fc1e-c15a-4f2c-bf02-54abfe17e68b.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240403/auto/storage/goog4_request%26X-Goog-Date%3D20240403T141910Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D60409e67715ee7ee494f0d9661e4b6933845a54b617f0a99c0a37e19ef6dd984d8b887a5c953706834ea9fed07bd4cb64a84d86d985b5135c0811ad541045772abfebdf5f7d3cae57421638117b09635f92d5e501e5bd0a1d492aeb5743ee17b2f2cd47d02ef6d596790a0f25e1742dabb00c222a54ef078d5a1e87ef13c72b9b4770b775b13b0dad6c4d751d27e52096724b0986ab5101c581cd8298db57f8bd5ab596ea3e794e9da7238584435881d9c53497fefc51eefc30c070dc0ccc284d2ff74f1d745dfc892c8a11ea3c405f6072dbfa0d84fd1a51b9a137bde54af8271ac4f44790bd08ad71c0e2f63719b01311fcc6be4ed0c383152f0c061571624
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'creditcardfraud:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F310%2F23498%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240403%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240403T141909Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D96298e3c00d53223c8e1c3e34757329b3b2c97b9fb394e746c795832fcd0eac679106bca3e462f2921d28a930c35f9376bf9ba2de19cbf5bf1a8595c1379abc5759003b452273747d59a2b9a3d31325bbcc71d543cb4a7d23c2df639ff29ea51d792f20b9d069f5376fc90ec4075384781d044eae5b220e2922275ca553303ed6054ba10f9d32ac8ce83af9edde78c12ffb289de36c53ab52151aa093ad1dfdff1acc17bf5a7f93700df3ddb1cb4d097b83291ad49da23f66775c8b6954bdcc6c390325412981069a1469796f65e7847c5fa91d447a3d79615004440e42a26c6d8f98e09764dcf18935ce4e158145fba02a0acff40b31b27eb718eb4d4d8b22f'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import numpy as np
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt

# Modelling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import IsolationForest

# Metrics
from sklearn.metrics import precision_score, recall_score, make_scorer

# Kaggle config.
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""### Importing the dataset
- Only the first `80,000` rows
"""

file_path = '/kaggle/input/creditcardfraud/creditcard.csv'
df = pd.read_csv(file_path)[:80000]

df.head()

"""## Preprocessing

### Features
- Extracting the **features** into `X` and **classes** into `y`
"""

X = df.drop(columns=['Time', 'Amount', 'Class']).values
y = df['Class'].values

X

"""### Target labels
- `1` denotes the **fraud** cases
- `0` denotes the **genuine** cases
"""

classes, classes_count = np.unique(df['Class'].values, return_counts=True)

plt.figure(figsize=(8, 6))
bars = plt.bar(classes, classes_count, color=['blue', 'red'])

for bar, count in zip(bars, classes_count):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 20, count,
             ha='center', va='bottom', color='black')

plt.title("Target labels")
plt.xticks(classes)

plt.xlabel("Classes")
plt.ylabel("Cases")
plt.legend(bars, ["Non-fraud", "Fraud"])

plt.show()

"""## Modelling

### Logistic Regression
- Using `LogisticRegression` for **binary classification**
"""

model = LogisticRegression(class_weight={0: 1, 1: 2}, max_iter=1000)

pred1 = model.fit(X, y).predict(X).sum()

print(f"Predicted number of Fraud cases : {pred1}")

"""### **Grid Search**
- Using `GridSearchCV` to determine the optimal **class weights**
- `4-fold` **Cross-Validation** is being used
"""

grid = GridSearchCV(
    estimator=LogisticRegression(max_iter=1000),
    param_grid={'class_weight': [{0: 1, 1: v} for v in range(1, 4)]},
    cv=4,
    n_jobs=-1)

grid

"""#### Training the `grid`"""

grid.fit(X, y)

"""#### Results of the `cross-validation`
- Finding out the optimal **class weights**
"""

cv_results = pd.DataFrame(grid.cv_results_)

cv_results.sort_values(by='mean_test_score', ascending=False).head(1)

"""## Metrics

### Precision and Recall
- Using different scoring metrics
"""

p_score = precision_score(y, grid.predict(X))
r_score = recall_score(y, grid.predict(X))

print(f"Precision Score : {p_score}\nRecall Score    : {r_score}")

"""#### Adding `precision` and `recall` to the **Grid Search**
- Optimizing over the `precision` score
"""

grid_pr = GridSearchCV(
    estimator=LogisticRegression(max_iter=1000),
    param_grid={'class_weight': [{0: 1, 1: v} for v in np.linspace(1, 20, 30)]},
    scoring={"precision": make_scorer(precision_score), "recall_score": make_scorer(recall_score)},
    refit='precision',
    return_train_score=True,
    cv=10,
    n_jobs=-1)

grid_pr

"""#### Training the `grid_pr`"""

grid_pr.fit(X, y)

"""#### Results of the `cross-validation`
- Finding out the optimal **class weights** based on `precision` score
"""

cv_results_pr = pd.DataFrame(grid_pr.cv_results_)

cv_results_pr.head()

"""#### **Best** mean `precision_score`"""

cv_results_pr.sort_values(by='mean_test_precision', ascending=False).head(1)

"""### Plotting the performance metrics
- We observe the **difference** in performance of the model on *training* vs. on the *testing* data
- The **intersection** b/w `Recall` and `Precision` occurs much **earlier** in **training** data

#### `Test` data
"""

plt.figure(figsize=(12, 4))

for score in ['mean_test_recall_score', 'mean_test_precision']:
    plt.plot([_[1] for _ in cv_results_pr['param_class_weight']], cv_results_pr[score], label=score)

plt.title("Class Weights vs. Testing Scores")
plt.xlabel("Class Weight")
plt.ylabel("Score")

plt.legend(["Mean Recall", "Mean Precision"])

plt.show()

"""#### `Train` data"""

plt.figure(figsize=(12, 4))

for score in ['mean_train_recall_score', 'mean_train_precision']:
    plt.plot([_[1] for _ in cv_results_pr['param_class_weight']], cv_results_pr[score], label=score)

plt.title("Class Weights vs. Training Scores")
plt.xlabel("Class Weight")
plt.ylabel("Score")

plt.legend(["Mean Recall", "Mean Precision"])

plt.show()

"""### New performance metric
- We will create a new metric that optimizes on the `min()` of **precision** and **recall** scores
"""

def min_precision_recall1(y_true, y_pred):
    p_score = precision_score(y_true, y_pred)
    r_score = recall_score(y_true, y_pred)

    return min(p_score, r_score)

"""### Creating and training the new **Grid**
- This makes use of the **new** metric
- Also, **transactions** are weighted by their `Amount`
"""

grid_pr2 = GridSearchCV(
    estimator=LogisticRegression(max_iter=1000),
    param_grid={'class_weight': [{0: 1, 1: v} for v in np.linspace(1, 20, 30)]},
    scoring={"precision": make_scorer(precision_score),
             "recall_score": make_scorer(recall_score),
             "min_recall_precision": make_scorer(min_precision_recall1)},
    refit='min_recall_precision',
    return_train_score=True,
    cv=10,
    n_jobs=-1)

grid_pr2.fit(X, y)

"""#### Results of the `cross-validation`"""

cv_results_pr2 = pd.DataFrame(grid_pr2.cv_results_)

cv_results_pr2.head()

"""#### **Best** of the new `score`"""

cv_results_pr2.sort_values(by='mean_test_min_recall_precision', ascending=False).head(1)

"""### Plotting the performance metrics

#### **Without** the weighted `Amount`
"""

plt.figure(figsize=(12, 4))

for score in ['mean_test_recall_score', 'mean_test_precision', 'mean_test_min_recall_precision']:
    plt.plot([_[1] for _ in cv_results_pr2['param_class_weight']], cv_results_pr2[score], label=score)

plt.title("Class Weights vs. Training Scores")
plt.xlabel("Class Weight")
plt.ylabel("Score")

plt.legend(["Mean Recall", "Mean Precision", "Minimum of Recall and Precision"])

plt.show()

"""#### **With** the weighted `Amount`"""

def min_precision_recall2(est, X, y_true, sample_weight=None):
    y_pred = est.predict(X)
    p_score = precision_score(y_true, y_pred)
    r_score = recall_score(y_true, y_pred)

    return min(p_score, r_score)

grid_pr2_2 = GridSearchCV(
    estimator=LogisticRegression(max_iter=1000),
    param_grid={'class_weight': [{0: 1, 1: v} for v in np.linspace(1, 20, 30)]},
    scoring={"precision": make_scorer(precision_score),
             "recall_score": make_scorer(recall_score),
             "min_recall_precision": min_precision_recall2},
    refit='min_recall_precision',
    return_train_score=True,
    cv=10,
    n_jobs=-1)

grid_pr2_2.fit(X, y, sample_weight=np.log(1 + df['Amount']))

cv_results_pr2_2 = pd.DataFrame(grid_pr2_2.cv_results_)

plt.figure(figsize=(12, 4))

for score in ['mean_test_recall_score', 'mean_test_precision', 'mean_test_min_recall_precision']:
    plt.plot([_[1] for _ in cv_results_pr2_2['param_class_weight']], cv_results_pr2_2[score], label=score)

plt.title("Class Weights vs. Training Scores")
plt.xlabel("Class Weight")
plt.ylabel("Score")

plt.legend(["Mean Recall", "Mean Precision", "Minimum of Recall and Precision"])

plt.show()

"""### Treating fraud cases as **outliers**
- Using the `IsolationForest` model
"""

model2 = IsolationForest().fit(X)

print(f"Non-outliers\t: {Counter(np.where(model2.predict(X) == -1, 1, 0))[0]}\nOutliers\t: {Counter(np.where(model2.predict(X) == -1, 1, 0))[1]}")

"""### Defining a **grid** for `IsolationForest`
- New scoring metrics i.e. we treat **fraud** cases as **outliers**
"""

def outlier_precision(mod, X, y):
    pred = mod.predict(X)
    return precision_score(y, np.where(pred==-1, 1, 0))

def outlier_recall(mod, X, y):
    pred = mod.predict(X)
    return recall_score(y, np.where(pred==-1, 1, 0))

grid_o = GridSearchCV(
    estimator=IsolationForest(),
    param_grid={
        'contamination': np.linspace(0.001, 0.02, 10)
    },
    scoring={
        'precision': outlier_precision,
        'recall': outlier_recall,
    },
    refit='precision',
    cv=5,
    n_jobs=-1
)

grid_o.fit(X, y)

"""#### Results of the `cross-validation`"""

cv_results_o = pd.DataFrame(grid_o.cv_results_)

cv_results_o.sort_values(by='mean_test_precision', ascending=False).head()

"""### Plotting the performance metrics"""

plt.figure(figsize=(12, 4))

for score in ['mean_test_recall', 'mean_test_precision']:
    plt.plot([_ for _ in cv_results_o['param_contamination']], cv_results_o[score], label=score)

plt.title("Contamination vs. Training Scores")
plt.xlabel("Contamination")
plt.ylabel("Score")

plt.legend(["Mean Recall", "Mean Precision"])

plt.show()

"""## Conclusion
Well, the `IsolationForest` model is of **no use**. We should stick to `Regression`.
"""